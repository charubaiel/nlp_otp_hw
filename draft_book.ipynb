{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/charubaiel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn import *\n",
    "import lightgbm as lgbm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "nltk.download(\"stopwords\")\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pos \\ neg - комменты с твиттера http://study.mokoron.com/ \n",
    "# labeled каггловский датасет по токсикам https://www.kaggle.com/blackmoon/russian-language-toxic-comments\n",
    "# Изначально взял только кагловский сет на 14 тыс. комментариев, но он оказался слишком специфичным\n",
    "# Проверить в ручную эти модели можно загрузив дампы из models\n",
    "# модели без номеров - обучены только на Kaggle датасете\n",
    "# модели с номером 2 - обучены на смешенном датасете\n",
    "# При обучении только на нем, модели выдавали под 83+% метрик, при чем не важно как делился сет, через кроссвалидацию от 2 - 20 цифры успешности оказывались слишком большими.\n",
    "# При ручном тестировании модель жутко косило в сторону токсичности, любые комменты она в большей степени относила к токсичным.\n",
    "# Для решения проблемы - решил достать нейтральные сеты схожей тематики и добавить их в виде \"не токсичных\"\n",
    "# Таким образом решая сразу и проблему объема выборки и однородности \\ плохой генерализации выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_neg = pd.read_csv('data/negative.csv',sep=';',header=None,usecols=[3])\n",
    "twitter_pos = pd.read_csv('data/positive.csv',sep=';',header=None,usecols=[3])\n",
    "vk_all = pd.read_csv('data/labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_toxic = vk_all.append(twitter_pos.rename(columns={3:'comment'}).sample(5000)).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Распределение специально формировалось не сильно дизбалансным, чтобы не решать отдельно проблему дисбаланса\n",
    "# В целом такой объем выборки сформировался эксперементальным путем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.751391\n",
       "1.0    0.248609\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttl_toxic['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,val_data,target,val_target = model_selection.train_test_split(ttl_toxic['comment'],ttl_toxic['toxic'],train_size=.75,stratify=ttl_toxic['toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stupid baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Первый этап - понять наш бейслайн, от чего мы может отталкиваться и какие в целом данные у нас есть.\n",
    "# Т.к. в наборе по большому счету только текст, без дополнительных признаков людей, EDA с генерацией фичей поможет очень слабо\n",
    "# По хорошему для формирования достаточно хороших прогнозов на сложных структурах имеет смысл использовать нейросети\n",
    "# Т.к. у нейросетей не маленький инференс + с нейросетями я знаком поверхностно для мвп и понятия возможностей начал с простого мл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = feature_extraction.text.TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для удобства использования, упрощения деплоя ,снижения захламленности и своеобразного инкапсулирования методов - все модельки сразу идут в пайплайны\n",
    "# Почти во всех моделях для упролщения работы добавлял распределение классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_base = linear_model.LogisticRegression(max_iter=1000,class_weight=target.value_counts(normalize=True).to_dict())\n",
    "pipe_base = pipeline.make_pipeline(tf,clf_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка проходит специально без какой то оптимизации гиперпараметров чтобы понять сырые возможности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] START .....................................................................\n",
      "[CV] END  f1: (test=0.695) precision: (test=0.764) recall: (test=0.638) total time=  12.6s\n",
      "[CV] START .....................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END  f1: (test=0.669) precision: (test=0.743) recall: (test=0.608) total time=  12.7s\n",
      "[CV] START .....................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   25.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END  f1: (test=0.710) precision: (test=0.779) recall: (test=0.652) total time=   9.5s\n",
      "[CV] START .....................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   34.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END  f1: (test=0.691) precision: (test=0.738) recall: (test=0.649) total time=  13.7s\n",
      "[CV] START .....................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   48.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END  f1: (test=0.682) precision: (test=0.756) recall: (test=0.621) total time=  14.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "scores['baseline'] = pd.DataFrame(model_selection.cross_validate(pipe_base,data,target,scoring=['f1','precision','recall'],cv=5,verbose=10)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для сравнения все CSV выборки пишу в словарь для конечной оценки методов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>12.519123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.099639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.689250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.755897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.633597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 baseline\n",
       "fit_time        12.519123\n",
       "score_time       0.099639\n",
       "test_f1          0.689250\n",
       "test_precision   0.755897\n",
       "test_recall      0.633597"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charubaiel/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=6, class_weight={0.0: 10940, 1.0: 3619},\n",
       "                                    max_iter=1000))])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_base.fit(data,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### work with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Первое что необходимо сделать - нормализовать текст, чтобы уменьшить кол-ва шума в данных\n",
    "# Наташа хоть и выглядит страшнее, но быстрее аналогов на порядок + имеет встроенные не плохие ембединги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "stopwords = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer (text):\n",
    "    words_only = re.sub('[^А-я]+',' ',text.lower())\n",
    "    doc = Doc(words_only)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    clean_text = []\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "        if (token.lemma not in stopwords) & (len(set(token.lemma))>1):\n",
    "            clean_text.append(token.lemma)\n",
    "            \n",
    "    return ' '.join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обрабатывать отдельно каждое предложение затратно за счет ресурсов на перепарсинг\n",
    "# Намного эффективнее подать на вход большой кусок текста, а потом расформировать его нобратно по комментам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = data.copy()\n",
    "old_text = ' жожо '.join(data)\n",
    "new_text = normalizer(old_text)\n",
    "data = pd.Series(new_text.split('жожо'))\n",
    "raw_val_data = val_data.copy()\n",
    "val_data = pd.Series(normalizer(' жожо '.join(val_data)).split('жожо'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_norm = linear_model.LogisticRegression(max_iter=1000,class_weight= target.value_counts(normalize=True).to_dict())\n",
    "pipe_norm = pipeline.make_pipeline(tf,clf_norm)\n",
    "scores['normalize'] = pd.DataFrame(model_selection.cross_validate(pipe_norm,data,target,scoring=['f1','precision','recall'],cv=5)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(class_weight={0.0: 0.7514252352496738,\n",
       "                                                  1.0: 0.24857476475032625},\n",
       "                                    max_iter=1000))])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_norm.fit(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Видны сразу изменения в балансе метрик и во времени обучения модели\n",
    "# Скорее всего такой буст бейслайну дал факт того, что выборки глобально отличаются и он просто нашел различия в стилистике между датасетами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>normalize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>12.519123</td>\n",
       "      <td>0.315521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.099639</td>\n",
       "      <td>0.046333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.689250</td>\n",
       "      <td>0.097987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.755897</td>\n",
       "      <td>0.983043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.633597</td>\n",
       "      <td>0.051677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 baseline  normalize\n",
       "fit_time        12.519123   0.315521\n",
       "score_time       0.099639   0.046333\n",
       "test_f1          0.689250   0.097987\n",
       "test_precision   0.755897   0.983043\n",
       "test_recall      0.633597   0.051677"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### micro EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda = vk_all.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попытка посмотреть на наши данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda['txt_len'] = eda['comment'].str.len()\n",
    "eda['txt_len_avg'] = eda['comment'].str.split().apply(lambda x: np.mean([len(word) for word in x]))\n",
    "eda['txt_words'] = eda['comment'].str.count(' ')\n",
    "eda['txt_puncts'] = eda['comment'].str.count('[^\\w^ ]')\n",
    "eda['txt_upper_cnt'] = eda['comment'].apply(lambda x: len([i for i in x if i.isupper()]))\n",
    "eda['txt_pct_upper'] = eda['txt_upper_cnt'] / eda['txt_len']\n",
    "eda['txt_pos_punc'] = eda['comment'].apply(lambda text: len(re.findall(r'\\)|D',text)))\n",
    "eda['txt_neg_punc'] = eda['comment'].apply(lambda text: len(re.findall(r'\\(|C|c|С|c',text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">txt_len</th>\n",
       "      <th colspan=\"2\" halign=\"left\">txt_len_avg</th>\n",
       "      <th colspan=\"2\" halign=\"left\">txt_words</th>\n",
       "      <th colspan=\"2\" halign=\"left\">txt_puncts</th>\n",
       "      <th colspan=\"2\" halign=\"left\">txt_upper_cnt</th>\n",
       "      <th colspan=\"2\" halign=\"left\">txt_pct_upper</th>\n",
       "      <th colspan=\"2\" halign=\"left\">txt_pos_punc</th>\n",
       "      <th colspan=\"2\" halign=\"left\">txt_neg_punc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>194.213332</td>\n",
       "      <td>274.750067</td>\n",
       "      <td>5.312308</td>\n",
       "      <td>1.063207</td>\n",
       "      <td>29.713436</td>\n",
       "      <td>40.811415</td>\n",
       "      <td>7.481849</td>\n",
       "      <td>9.135183</td>\n",
       "      <td>3.984978</td>\n",
       "      <td>7.812980</td>\n",
       "      <td>0.023631</td>\n",
       "      <td>0.027502</td>\n",
       "      <td>0.279261</td>\n",
       "      <td>0.709825</td>\n",
       "      <td>0.499478</td>\n",
       "      <td>1.353467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>141.392665</td>\n",
       "      <td>261.776417</td>\n",
       "      <td>5.467866</td>\n",
       "      <td>1.466984</td>\n",
       "      <td>21.449233</td>\n",
       "      <td>42.106635</td>\n",
       "      <td>5.972234</td>\n",
       "      <td>9.188326</td>\n",
       "      <td>6.595939</td>\n",
       "      <td>26.932055</td>\n",
       "      <td>0.052774</td>\n",
       "      <td>0.130923</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>0.434023</td>\n",
       "      <td>0.454414</td>\n",
       "      <td>1.689128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          txt_len             txt_len_avg            txt_words             \\\n",
       "             mean         std        mean       std       mean        std   \n",
       "toxic                                                                       \n",
       "0.0    194.213332  274.750067    5.312308  1.063207  29.713436  40.811415   \n",
       "1.0    141.392665  261.776417    5.467866  1.466984  21.449233  42.106635   \n",
       "\n",
       "      txt_puncts           txt_upper_cnt            txt_pct_upper            \\\n",
       "            mean       std          mean        std          mean       std   \n",
       "toxic                                                                         \n",
       "0.0     7.481849  9.135183      3.984978   7.812980      0.023631  0.027502   \n",
       "1.0     5.972234  9.188326      6.595939  26.932055      0.052774  0.130923   \n",
       "\n",
       "      txt_pos_punc           txt_neg_punc            \n",
       "              mean       std         mean       std  \n",
       "toxic                                                \n",
       "0.0       0.279261  0.709825     0.499478  1.353467  \n",
       "1.0       0.094488  0.434023     0.454414  1.689128  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda.groupby('toxic')[eda.filter(regex='txt').columns].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>txt_len</th>\n",
       "      <th>txt_len_avg</th>\n",
       "      <th>txt_words</th>\n",
       "      <th>txt_puncts</th>\n",
       "      <th>txt_upper_cnt</th>\n",
       "      <th>txt_pct_upper</th>\n",
       "      <th>txt_pos_punc</th>\n",
       "      <th>txt_neg_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.091782</td>\n",
       "      <td>0.060394</td>\n",
       "      <td>-0.094138</td>\n",
       "      <td>-0.077608</td>\n",
       "      <td>0.072997</td>\n",
       "      <td>0.171514</td>\n",
       "      <td>-0.136895</td>\n",
       "      <td>-0.014424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_len</th>\n",
       "      <td>-0.091782</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024703</td>\n",
       "      <td>0.991756</td>\n",
       "      <td>0.924170</td>\n",
       "      <td>0.370052</td>\n",
       "      <td>-0.047513</td>\n",
       "      <td>0.382291</td>\n",
       "      <td>0.529863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_len_avg</th>\n",
       "      <td>0.060394</td>\n",
       "      <td>0.024703</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025150</td>\n",
       "      <td>0.016616</td>\n",
       "      <td>0.022158</td>\n",
       "      <td>0.079450</td>\n",
       "      <td>-0.001665</td>\n",
       "      <td>0.037176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_words</th>\n",
       "      <td>-0.094138</td>\n",
       "      <td>0.991756</td>\n",
       "      <td>-0.025150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929034</td>\n",
       "      <td>0.368631</td>\n",
       "      <td>-0.049015</td>\n",
       "      <td>0.378389</td>\n",
       "      <td>0.514993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_puncts</th>\n",
       "      <td>-0.077608</td>\n",
       "      <td>0.924170</td>\n",
       "      <td>0.016616</td>\n",
       "      <td>0.929034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.346096</td>\n",
       "      <td>-0.033393</td>\n",
       "      <td>0.418016</td>\n",
       "      <td>0.497885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_upper_cnt</th>\n",
       "      <td>0.072997</td>\n",
       "      <td>0.370052</td>\n",
       "      <td>0.022158</td>\n",
       "      <td>0.368631</td>\n",
       "      <td>0.346096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.565105</td>\n",
       "      <td>0.115176</td>\n",
       "      <td>0.692265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_pct_upper</th>\n",
       "      <td>0.171514</td>\n",
       "      <td>-0.047513</td>\n",
       "      <td>0.079450</td>\n",
       "      <td>-0.049015</td>\n",
       "      <td>-0.033393</td>\n",
       "      <td>0.565105</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.040094</td>\n",
       "      <td>0.312005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_pos_punc</th>\n",
       "      <td>-0.136895</td>\n",
       "      <td>0.382291</td>\n",
       "      <td>-0.001665</td>\n",
       "      <td>0.378389</td>\n",
       "      <td>0.418016</td>\n",
       "      <td>0.115176</td>\n",
       "      <td>-0.040094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt_neg_punc</th>\n",
       "      <td>-0.014424</td>\n",
       "      <td>0.529863</td>\n",
       "      <td>0.037176</td>\n",
       "      <td>0.514993</td>\n",
       "      <td>0.497885</td>\n",
       "      <td>0.692265</td>\n",
       "      <td>0.312005</td>\n",
       "      <td>0.395420</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toxic   txt_len  txt_len_avg  txt_words  txt_puncts  \\\n",
       "toxic          1.000000 -0.091782     0.060394  -0.094138   -0.077608   \n",
       "txt_len       -0.091782  1.000000     0.024703   0.991756    0.924170   \n",
       "txt_len_avg    0.060394  0.024703     1.000000  -0.025150    0.016616   \n",
       "txt_words     -0.094138  0.991756    -0.025150   1.000000    0.929034   \n",
       "txt_puncts    -0.077608  0.924170     0.016616   0.929034    1.000000   \n",
       "txt_upper_cnt  0.072997  0.370052     0.022158   0.368631    0.346096   \n",
       "txt_pct_upper  0.171514 -0.047513     0.079450  -0.049015   -0.033393   \n",
       "txt_pos_punc  -0.136895  0.382291    -0.001665   0.378389    0.418016   \n",
       "txt_neg_punc  -0.014424  0.529863     0.037176   0.514993    0.497885   \n",
       "\n",
       "               txt_upper_cnt  txt_pct_upper  txt_pos_punc  txt_neg_punc  \n",
       "toxic               0.072997       0.171514     -0.136895     -0.014424  \n",
       "txt_len             0.370052      -0.047513      0.382291      0.529863  \n",
       "txt_len_avg         0.022158       0.079450     -0.001665      0.037176  \n",
       "txt_words           0.368631      -0.049015      0.378389      0.514993  \n",
       "txt_puncts          0.346096      -0.033393      0.418016      0.497885  \n",
       "txt_upper_cnt       1.000000       0.565105      0.115176      0.692265  \n",
       "txt_pct_upper       0.565105       1.000000     -0.040094      0.312005  \n",
       "txt_pos_punc        0.115176      -0.040094      1.000000      0.395420  \n",
       "txt_neg_punc        0.692265       0.312005      0.395420      1.000000  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda.iloc[:,1:].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант с дорбавлениями фичей для модели, исходя из EDA есть слабый шанс на улучшение, без особой потери в скорости \\ инференсе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_from_text(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    df.columns=['text']\n",
    "    df['txt_len'] = df['text'].str.len()\n",
    "    df['txt_len_avg'] = df['text'].str.split().apply(lambda x: np.mean([len(word) for word in x]))\n",
    "    df['txt_words'] = df['text'].str.count(' ')\n",
    "    df['txt_puncts'] = df['text'].str.count('[^\\w^ ]')\n",
    "    df['txt_upper_cnt'] = df['text'].apply(lambda x: len([i for i in x if i.isupper()]))\n",
    "    df['txt_pos_punc'] = df['text'].apply(lambda text: len(re.findall(r'\\)|D',text)))\n",
    "    df['txt_neg_punc'] = df['text'].apply(lambda text: len(re.findall(r'\\(|C|c|С|c',text)))\n",
    "    \n",
    "    return df.drop('text',axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_tf = feature_extraction.text.TfidfVectorizer()\n",
    "fe_tf.fit(data,target)\n",
    "new_fe=preprocessing.FunctionTransformer(features_from_text)\n",
    "text_preproc = pipeline.FeatureUnion([('idf',fe_tf),('fe',new_fe)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fe = linear_model.LogisticRegression(max_iter=1000,class_weight= target.value_counts(normalize=True).to_dict())\n",
    "pipe_fe = pipeline.Pipeline([('preproc',text_preproc),('clf',clf_fe)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "scores['features'] = pd.DataFrame(model_selection.cross_validate(pipe_fe,data,target,scoring=['f1','precision','recall'],cv=5)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preproc',\n",
       "                 FeatureUnion(transformer_list=[('idf', TfidfVectorizer()),\n",
       "                                                ('fe',\n",
       "                                                 FunctionTransformer(func=<function features_from_text at 0x7fb4f9918dc0>))])),\n",
       "                ('clf',\n",
       "                 LogisticRegression(class_weight={0.0: 0.7514252352496738,\n",
       "                                                  1.0: 0.24857476475032625},\n",
       "                                    max_iter=1000))])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_fe.fit(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>normalize</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>12.519123</td>\n",
       "      <td>0.315521</td>\n",
       "      <td>1.081329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.099639</td>\n",
       "      <td>0.046333</td>\n",
       "      <td>0.143306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.689250</td>\n",
       "      <td>0.097987</td>\n",
       "      <td>0.103340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.755897</td>\n",
       "      <td>0.983043</td>\n",
       "      <td>0.966164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.633597</td>\n",
       "      <td>0.051677</td>\n",
       "      <td>0.054716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 baseline  normalize  features\n",
       "fit_time        12.519123   0.315521  1.081329\n",
       "score_time       0.099639   0.046333  0.143306\n",
       "test_f1          0.689250   0.097987  0.103340\n",
       "test_precision   0.755897   0.983043  0.966164\n",
       "test_recall      0.633597   0.051677  0.054716"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Готовые векторные пространства имеют больший диапазон слов, за счет чего могут быть эффективнее преобразования локальных сетов данных\n",
    "# + они сформированны на более сложных алгоритмах, что дает бОльшую генерализующую и контекстную силу\n",
    "# конкретно навекрвские вектора весят не много, и дают сравнимый эффект с крутыми собратьями\n",
    "# https://github.com/natasha/navec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav = Navec.load('models/emb_navec.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence_list):\n",
    "    vectors = []\n",
    "    for sentence in sentence_list:\n",
    "        sent_vec = []\n",
    "        for i in sentence.split():\n",
    "            if i in nav:\n",
    "                sent_vec.append(nav[i])\n",
    "            else:\n",
    "                sent_vec.append(nav['<unk>'])\n",
    "        if sentence.strip() == '':\n",
    "            sent_vec = [nav['<unk>']]\n",
    "        vectors.append(np.mean(sent_vec,axis=0))\n",
    "    return np.vstack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_func = preprocessing.FunctionTransformer(get_sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_vec = linear_model.LogisticRegression(max_iter=1000,class_weight= target.value_counts(normalize=True).to_dict())\n",
    "pipe_vec = pipeline.make_pipeline(vec_func,clf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['sample_vec'] = pd.DataFrame(model_selection.cross_validate(pipe_vec,data,target,scoring=['f1','precision','recall'],cv=5)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('functiontransformer',\n",
       "                 FunctionTransformer(func=<function get_sentence_vector at 0x7fb4e016c790>)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(class_weight={0.0: 0.7514252352496738,\n",
       "                                                  1.0: 0.24857476475032625},\n",
       "                                    max_iter=1000))])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_vec.fit(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>normalize</th>\n",
       "      <th>features</th>\n",
       "      <th>sample_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>12.519123</td>\n",
       "      <td>0.315521</td>\n",
       "      <td>1.081329</td>\n",
       "      <td>1.405890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.099639</td>\n",
       "      <td>0.046333</td>\n",
       "      <td>0.143306</td>\n",
       "      <td>0.309307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.689250</td>\n",
       "      <td>0.097987</td>\n",
       "      <td>0.103340</td>\n",
       "      <td>0.456973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.755897</td>\n",
       "      <td>0.983043</td>\n",
       "      <td>0.966164</td>\n",
       "      <td>0.833795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.633597</td>\n",
       "      <td>0.051677</td>\n",
       "      <td>0.054716</td>\n",
       "      <td>0.315007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 baseline  normalize  features  sample_vec\n",
       "fit_time        12.519123   0.315521  1.081329    1.405890\n",
       "score_time       0.099639   0.046333  0.143306    0.309307\n",
       "test_f1          0.689250   0.097987  0.103340    0.456973\n",
       "test_precision   0.755897   0.983043  0.966164    0.833795\n",
       "test_recall      0.633597   0.051677  0.054716    0.315007"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Один из способов собрать по дешевке fasttext на коленях использовать для векторизации отдельно слова, и отдельно составные их части от 2-5 букв\n",
    "# в теории можно найти частоиспользуемые корневые вещи по типу \"бл\", \"еб\" от мата и тд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = feature_extraction.text.TfidfVectorizer()\n",
    "word_vectorizer.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', min_df=3, ngram_range=(2, 4),\n",
       "                sublinear_tf=True)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vectorizer = feature_extraction.text.TfidfVectorizer(\n",
    "    min_df=3,\n",
    "    sublinear_tf=True,\n",
    "    analyzer='char',\n",
    "    ngram_range=(2,4))\n",
    "char_vectorizer.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_fu = pipeline.FeatureUnion([('idf_w',word_vectorizer),('idf_c',char_vectorizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_2idf = linear_model.LogisticRegression(max_iter=1000,C=6,class_weight= target.value_counts(normalize=True).to_dict())\n",
    "pipe_idf_fe = pipeline.Pipeline([('idf',idf_fu),('clf',clf_2idf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['idf_features'] = pd.DataFrame(model_selection.cross_validate(pipe_idf_fe,data,target,scoring=['f1','precision','recall'],cv=5)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('idf',\n",
       "                 FeatureUnion(transformer_list=[('idf_w', TfidfVectorizer()),\n",
       "                                                ('idf_c',\n",
       "                                                 TfidfVectorizer(analyzer='char',\n",
       "                                                                 min_df=3,\n",
       "                                                                 ngram_range=(2,\n",
       "                                                                              4),\n",
       "                                                                 sublinear_tf=True))])),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=6,\n",
       "                                    class_weight={0.0: 0.7514252352496738,\n",
       "                                                  1.0: 0.24857476475032625},\n",
       "                                    max_iter=1000))])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_idf_fe.fit(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>normalize</th>\n",
       "      <th>features</th>\n",
       "      <th>sample_vec</th>\n",
       "      <th>idf_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>12.519123</td>\n",
       "      <td>0.315521</td>\n",
       "      <td>1.081329</td>\n",
       "      <td>1.405890</td>\n",
       "      <td>4.759447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.099639</td>\n",
       "      <td>0.046333</td>\n",
       "      <td>0.143306</td>\n",
       "      <td>0.309307</td>\n",
       "      <td>0.482817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.689250</td>\n",
       "      <td>0.097987</td>\n",
       "      <td>0.103340</td>\n",
       "      <td>0.456973</td>\n",
       "      <td>0.625519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.755897</td>\n",
       "      <td>0.983043</td>\n",
       "      <td>0.966164</td>\n",
       "      <td>0.833795</td>\n",
       "      <td>0.930100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.633597</td>\n",
       "      <td>0.051677</td>\n",
       "      <td>0.054716</td>\n",
       "      <td>0.315007</td>\n",
       "      <td>0.471409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 baseline  normalize  features  sample_vec  idf_features\n",
       "fit_time        12.519123   0.315521  1.081329    1.405890      4.759447\n",
       "score_time       0.099639   0.046333  0.143306    0.309307      0.482817\n",
       "test_f1          0.689250   0.097987  0.103340    0.456973      0.625519\n",
       "test_precision   0.755897   0.983043  0.966164    0.833795      0.930100\n",
       "test_recall      0.633597   0.051677  0.054716    0.315007      0.471409"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blend models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для генерализации ответов, один из вариантов это блендинг моделей разного рода\n",
    "# Тестирую разнородные модели, потом из них беру только те, которые показали сравнительно не плохой результат\n",
    "# Или выделились по конкретным метрикам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_func = preprocessing.FunctionTransformer(get_sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prior=target.value_counts(normalize=True).values[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr_vec = linear_model.LogisticRegression(max_iter=1000,C=6,class_weight= target.value_counts().to_dict())\n",
    "clf_nb_vec = naive_bayes.BernoulliNB(class_prior=class_prior)\n",
    "clf_knn_vec = neighbors.KNeighborsClassifier(30)\n",
    "clf_svc_vec = linear_model.SGDClassifier(loss='modified_huber',class_weight= target.value_counts().to_dict())\n",
    "clf_rf_vec = lgbm.LGBMClassifier(n_estimators=1500,learning_rate=0.07,num_leaves=15,class_weight= target.value_counts().to_dict())\n",
    "clf_mlp_vec = neural_network.MLPClassifier(hidden_layer_sizes=(300,1),max_iter=1000,learning_rate='adaptive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:10, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time          1.732373\n",
      "score_time        0.330093\n",
      "test_f1           0.483680\n",
      "test_precision    0.813432\n",
      "test_recall       0.344575\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:17,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time          1.207198\n",
      "score_time        0.319870\n",
      "test_f1           0.597102\n",
      "test_precision    0.521148\n",
      "test_recall       0.699088\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:28,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time          1.197858\n",
      "score_time        0.826378\n",
      "test_f1           0.657408\n",
      "test_precision    0.596258\n",
      "test_recall       0.732791\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:39, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time          2.056230\n",
      "score_time        0.304323\n",
      "test_f1           0.485741\n",
      "test_precision    0.690036\n",
      "test_recall       0.398731\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:19, 20.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time          7.515308\n",
      "score_time        0.327282\n",
      "test_f1           0.672619\n",
      "test_precision    0.818714\n",
      "test_recall       0.570883\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [02:26, 24.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time          13.207715\n",
      "score_time         0.308060\n",
      "test_f1            0.706715\n",
      "test_precision     0.735303\n",
      "test_recall        0.682252\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for n,model in tqdm(enumerate([clf_lr_vec,clf_nb_vec,clf_knn_vec,clf_svc_vec,clf_rf_vec,clf_mlp_vec])):\n",
    "    tmp_pipe = pipeline.make_pipeline(vec_func,model)\n",
    "    models[n] = pd.DataFrame(model_selection.cross_validate(tmp_pipe,data,target,scoring=['f1','precision','recall'],cv=5)).mean()\n",
    "    print(models[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>13.207715</td>\n",
       "      <td>0.308060</td>\n",
       "      <td>0.706715</td>\n",
       "      <td>0.735303</td>\n",
       "      <td>0.682252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbm</th>\n",
       "      <td>7.515308</td>\n",
       "      <td>0.327282</td>\n",
       "      <td>0.672619</td>\n",
       "      <td>0.818714</td>\n",
       "      <td>0.570883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>1.197858</td>\n",
       "      <td>0.826378</td>\n",
       "      <td>0.657408</td>\n",
       "      <td>0.596258</td>\n",
       "      <td>0.732791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb</th>\n",
       "      <td>1.207198</td>\n",
       "      <td>0.319870</td>\n",
       "      <td>0.597102</td>\n",
       "      <td>0.521148</td>\n",
       "      <td>0.699088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svc</th>\n",
       "      <td>2.056230</td>\n",
       "      <td>0.304323</td>\n",
       "      <td>0.485741</td>\n",
       "      <td>0.690036</td>\n",
       "      <td>0.398731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>1.732373</td>\n",
       "      <td>0.330093</td>\n",
       "      <td>0.483680</td>\n",
       "      <td>0.813432</td>\n",
       "      <td>0.344575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fit_time  score_time   test_f1  test_precision  test_recall\n",
       "mlp  13.207715    0.308060  0.706715        0.735303     0.682252\n",
       "gbm   7.515308    0.327282  0.672619        0.818714     0.570883\n",
       "knn   1.197858    0.826378  0.657408        0.596258     0.732791\n",
       "nb    1.207198    0.319870  0.597102        0.521148     0.699088\n",
       "svc   2.056230    0.304323  0.485741        0.690036     0.398731\n",
       "lr    1.732373    0.330093  0.483680        0.813432     0.344575"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(models).rename(columns = {0:'lr',1:'nb',2:'knn',3:'svc',4:'gbm',5:'mlp'}).T.sort_values(by='test_f1',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лучшие собрал в классифаер, который выбирает самый \"уверенный\" ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vote_vec = ensemble.VotingClassifier(estimators=[('knn',clf_knn_vec),('gbm',clf_rf_vec),('mlp',clf_mlp_vec)],voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_vote_vec = pipeline.make_pipeline(vec_func,vote_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['vote_models'] = pd.DataFrame(model_selection.cross_validate(pipe_vote_vec,data,target,scoring=['f1','precision','recall'],cv=5,n_jobs=4)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('functiontransformer',\n",
       "                 FunctionTransformer(func=<function get_sentence_vector at 0x7fb4e016c790>)),\n",
       "                ('votingclassifier',\n",
       "                 VotingClassifier(estimators=[('knn',\n",
       "                                               KNeighborsClassifier(n_neighbors=30)),\n",
       "                                              ('gbm',\n",
       "                                               LGBMClassifier(class_weight={0.0: 10940,\n",
       "                                                                            1.0: 3619},\n",
       "                                                              learning_rate=0.07,\n",
       "                                                              n_estimators=1500,\n",
       "                                                              num_leaves=15)),\n",
       "                                              ('mlp',\n",
       "                                               MLPClassifier(hidden_layer_sizes=(300,\n",
       "                                                                                 1),\n",
       "                                                             learning_rate='adaptive',\n",
       "                                                             max_iter=1000))],\n",
       "                                  voting='soft'))])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_vote_vec.fit(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>normalize</th>\n",
       "      <th>features</th>\n",
       "      <th>sample_vec</th>\n",
       "      <th>idf_features</th>\n",
       "      <th>vote_models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>12.519123</td>\n",
       "      <td>0.315521</td>\n",
       "      <td>1.081329</td>\n",
       "      <td>1.405890</td>\n",
       "      <td>4.759447</td>\n",
       "      <td>38.413581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.099639</td>\n",
       "      <td>0.046333</td>\n",
       "      <td>0.143306</td>\n",
       "      <td>0.309307</td>\n",
       "      <td>0.482817</td>\n",
       "      <td>1.240588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.689250</td>\n",
       "      <td>0.097987</td>\n",
       "      <td>0.103340</td>\n",
       "      <td>0.456973</td>\n",
       "      <td>0.625519</td>\n",
       "      <td>0.700482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.755897</td>\n",
       "      <td>0.983043</td>\n",
       "      <td>0.966164</td>\n",
       "      <td>0.833795</td>\n",
       "      <td>0.930100</td>\n",
       "      <td>0.784968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.633597</td>\n",
       "      <td>0.051677</td>\n",
       "      <td>0.054716</td>\n",
       "      <td>0.315007</td>\n",
       "      <td>0.471409</td>\n",
       "      <td>0.634994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 baseline  normalize  features  sample_vec  idf_features  \\\n",
       "fit_time        12.519123   0.315521  1.081329    1.405890      4.759447   \n",
       "score_time       0.099639   0.046333  0.143306    0.309307      0.482817   \n",
       "test_f1          0.689250   0.097987  0.103340    0.456973      0.625519   \n",
       "test_precision   0.755897   0.983043  0.966164    0.833795      0.930100   \n",
       "test_recall      0.633597   0.051677  0.054716    0.315007      0.471409   \n",
       "\n",
       "                vote_models  \n",
       "fit_time          38.413581  \n",
       "score_time         1.240588  \n",
       "test_f1            0.700482  \n",
       "test_precision     0.784968  \n",
       "test_recall        0.634994  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blend results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# До до этого блендили модели, еще один вариант сблендить подходы к моделированию\n",
    "# При объединении векторного подхода + собвственных idf ных , результат должен быть точнее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def voting(sentences,func=np.max):\n",
    "    sentences = [normalizer(txt) for txt in sentences]\n",
    "    probs = []\n",
    "    probs.append(pipe_idf_fe.predict_proba(sentences)[:,1])\n",
    "    probs.append(pipe_vote_vec.predict_proba(sentences)[:,1])\n",
    "    probs.append(pipe_vec.predict_proba(sentences)[:,1])\n",
    "    return np.apply_over_axes(func,np.array(probs),axes=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/charubaiel/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "eval_data = defaultdict(list)\n",
    "for model in [pipe_norm,pipe_fe,pipe_idf_fe,pipe_vec,pipe_vote_vec]:\n",
    "    eval_data['f1'].append(metrics.f1_score(val_target,model.predict(val_data)))\n",
    "    eval_data['recall'].append(metrics.recall_score(val_target,model.predict(val_data)))\n",
    "    eval_data['precision'].append(metrics.precision_score(val_target,model.predict(val_data)))\n",
    "    eval_data['roc_auc'].append(metrics.roc_auc_score(val_target,model.predict(val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data['f1'].append(metrics.f1_score(val_target,voting(val_data).round()))\n",
    "eval_data['recall'].append(metrics.recall_score(val_target,voting(val_data).round()))\n",
    "eval_data['precision'].append(metrics.precision_score(val_target,voting(val_data).round()))\n",
    "eval_data['roc_auc'].append(metrics.roc_auc_score(val_target,voting(val_data).round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data['f1'].append(metrics.f1_score(val_target,voting(val_data,func=np.mean).round()))\n",
    "eval_data['recall'].append(metrics.recall_score(val_target,voting(val_data,func=np.mean).round()))\n",
    "eval_data['precision'].append(metrics.precision_score(val_target,voting(val_data,func=np.mean).round()))\n",
    "eval_data['roc_auc'].append(metrics.roc_auc_score(val_target,voting(val_data,func=np.mean).round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Итог радует\n",
    "# Хоть и не получили супргероя который выполняет сразу обе задачи ( и полноту и сложность)\n",
    "# Но зато есть несколько вариантов для выбора исходя из реальной задачи (инференс \\ точность \\ полнота)\n",
    "# Если говорить о метриках, то "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blend_max</th>\n",
       "      <td>0.740933</td>\n",
       "      <td>0.710853</td>\n",
       "      <td>0.773670</td>\n",
       "      <td>0.821005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vote_vec</th>\n",
       "      <td>0.701975</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.765916</td>\n",
       "      <td>0.791168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blend_mean</th>\n",
       "      <td>0.636990</td>\n",
       "      <td>0.497929</td>\n",
       "      <td>0.883824</td>\n",
       "      <td>0.738131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idf_df</th>\n",
       "      <td>0.625753</td>\n",
       "      <td>0.473074</td>\n",
       "      <td>0.923948</td>\n",
       "      <td>0.730091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vec</th>\n",
       "      <td>0.445921</td>\n",
       "      <td>0.305717</td>\n",
       "      <td>0.823661</td>\n",
       "      <td>0.642025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fe</th>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.053853</td>\n",
       "      <td>0.970149</td>\n",
       "      <td>0.526652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.045568</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.522372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  f1    recall  precision   roc_auc\n",
       "blend_max   0.740933  0.710853   0.773670  0.821005\n",
       "vote_vec    0.701975  0.647887   0.765916  0.791168\n",
       "blend_mean  0.636990  0.497929   0.883824  0.738131\n",
       "idf_df      0.625753  0.473074   0.923948  0.730091\n",
       "vec         0.445921  0.305717   0.823661  0.642025\n",
       "fe          0.102041  0.053853   0.970149  0.526652\n",
       "norm        0.086957  0.045568   0.948276  0.522372"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(eval_data,index=['norm','fe','idf_df','vec','vote_vec','blend_max','blend_mean']).sort_values(by='f1',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Это проходные результаты, которы можно еще доработать \\ проверить \\ затюнить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_text (text):\n",
    "    return pd.Series({'pure_idf_model' : pipe_norm.predict_proba([text])[:,1][0],\n",
    "    'some_features_model' : pipe_fe.predict_proba([text])[:,1][0],\n",
    "    'double_idf_model' :pipe_idf_fe.predict_proba([text])[:,1][0],\n",
    "    'pure_vectors_model' :pipe_vec.predict_proba([text])[:,1][0],\n",
    "    'blending_models' : pipe_vote_vec.predict_proba([text])[:,1][0],\n",
    "    'blending_ttl_mean' : voting([text],func=np.mean)[0],\n",
    "    'blending_ttl_max' : voting([text],func=np.max)[0]},name='Степень токсичности')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_predict(model,text):\n",
    "    try:\n",
    "        return model.predict_proba([text])[:,1][0]\n",
    "    except:\n",
    "        return model([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим на инференс и решаем что будем использовать\n",
    "# Так же параллельно руками проверяем работоспособность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562 µs ± 16.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "time_predict(pipe_norm,'Началось все с заселения! Минут 20 искали бронь - с трудом нашли. В документах ошибка на ошибке, пришлось несколько раз переделывать. Девушка на ресепшене с трудом с помощью калькулятора смогла посчитать сколько надо сдачу дать, но при этом все равно умудрилась обмануть и при этом с наивным взглядом, протягивая эти 100 рублей выдавила из себя: «ну я же не обманула». ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.73 ms ± 132 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "time_predict(pipe_fe,'Началось все с заселения! Минут 20 искали бронь - с трудом нашли. В документах ошибка на ошибке, пришлось несколько раз переделывать. Девушка на ресепшене с трудом с помощью калькулятора смогла посчитать сколько надо сдачу дать, но при этом все равно умудрилась обмануть и при этом с наивным взглядом, протягивая эти 100 рублей выдавила из себя: «ну я же не обманула». ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21 ms ± 110 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "time_predict(pipe_idf_fe,'Началось все с заселения! Минут 20 искали бронь - с трудом нашли. В документах ошибка на ошибке, пришлось несколько раз переделывать. Девушка на ресепшене с трудом с помощью калькулятора смогла посчитать сколько надо сдачу дать, но при этом все равно умудрилась обмануть и при этом с наивным взглядом, протягивая эти 100 рублей выдавила из себя: «ну я же не обманула». ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460 µs ± 2.49 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "time_predict(pipe_vec,'Началось все с заселения! Минут 20 искали бронь - с трудом нашли. В документах ошибка на ошибке, пришлось несколько раз переделывать. Девушка на ресепшене с трудом с помощью калькулятора смогла посчитать сколько надо сдачу дать, но при этом все равно умудрилась обмануть и при этом с наивным взглядом, протягивая эти 100 рублей выдавила из себя: «ну я же не обманула». ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.5 ms ± 2.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "time_predict(pipe_vote_vec,'Началось все с заселения! Минут 20 искали бронь - с трудом нашли. В документах ошибка на ошибке, пришлось несколько раз переделывать. Девушка на ресепшене с трудом с помощью калькулятора смогла посчитать сколько надо сдачу дать, но при этом все равно умудрилась обмануть и при этом с наивным взглядом, протягивая эти 100 рублей выдавила из себя: «ну я же не обманула». ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.7 ms ± 3.49 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "time_predict(voting,'Началось все с заселения! Минут 20 искали бронь - с трудом нашли. В документах ошибка на ошибке, пришлось несколько раз переделывать. Девушка на ресепшене с трудом с помощью калькулятора смогла посчитать сколько надо сдачу дать, но при этом все равно умудрилась обмануть и при этом с наивным взглядом, протягивая эти 100 рублей выдавила из себя: «ну я же не обманула». ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 ms ± 11.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "check_text('Началось все с заселения! Минут 20 искали бронь - с трудом нашли. В документах ошибка на ошибке, пришлось несколько раз переделывать. Девушка на ресепшене с трудом с помощью калькулятора смогла посчитать сколько надо сдачу дать, но при этом все равно умудрилась обмануть и при этом с наивным взглядом, протягивая эти 100 рублей выдавила из себя: «ну я же не обманула». ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_50e94_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Степень токсичности</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_50e94_level0_row0\" class=\"row_heading level0 row0\" >pure_idf_model</th>\n",
       "      <td id=\"T_50e94_row0_col0\" class=\"data row0 col0\" >13.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50e94_level0_row1\" class=\"row_heading level0 row1\" >some_features_model</th>\n",
       "      <td id=\"T_50e94_row1_col0\" class=\"data row1 col0\" >13.04%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50e94_level0_row2\" class=\"row_heading level0 row2\" >double_idf_model</th>\n",
       "      <td id=\"T_50e94_row2_col0\" class=\"data row2 col0\" >17.59%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50e94_level0_row3\" class=\"row_heading level0 row3\" >pure_vectors_model</th>\n",
       "      <td id=\"T_50e94_row3_col0\" class=\"data row3 col0\" >51.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50e94_level0_row4\" class=\"row_heading level0 row4\" >blending_models</th>\n",
       "      <td id=\"T_50e94_row4_col0\" class=\"data row4 col0\" >68.27%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50e94_level0_row5\" class=\"row_heading level0 row5\" >blending_ttl_mean</th>\n",
       "      <td id=\"T_50e94_row5_col0\" class=\"data row5 col0\" >54.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50e94_level0_row6\" class=\"row_heading level0 row6\" >blending_ttl_max</th>\n",
       "      <td id=\"T_50e94_row6_col0\" class=\"data row6 col0\" >81.60%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb4e9a58ca0>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_text('Эх, жалко что ты такой безнадежный').to_frame().style.format('{:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipe_vec,'models/model_vec_2.joblib')\n",
    "joblib.dump(pipe_idf_fe,'models/model_idf_2.joblib')\n",
    "joblib.dump(pipe_vote_vec,'models/model_vec_vote_2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7801a1fce19b4dbca8b7bce4059e552648312323ed9d108616a9100a6d7a4a9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
